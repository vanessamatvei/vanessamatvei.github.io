<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Vanessa Matvei | Articles</title>
  <link rel="stylesheet" type="text/css" href="style.css">
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
</head>
<body>
  <!-- Background canvas for petals -->
  <canvas id="bg-canvas"></canvas>

  <nav>
      <a href="index.html" class="nav-btn">About</a>        
      <a href="publications.html" class="nav-btn">Publications</a>      
      <a href="articles.html" class="nav-btn nav-active">Articles</a>
      <a href="contact.html" class="nav-btn">Contact</a>
  </nav>

  <header class="hero">
    <h1 class="hero-title">Articles & Essays</h1>
  </header>

  <main class="panel reveal">
    <section id="articles">
      <article>
        <h2><b>How can we deal with algorithmic bias and opacity?</b></h2>
        <p><b>Introduction</b></p>
        <p>
          Nowadays, algorithms and AI systems increasingly shape decisions in areas ranging from 
          credit scoring and hiring to news feeds and criminal justice. While efficient, these systems can encode and amplify 
          unfair biases. IBM defines <u>algorithmic bias</u> as “systematic errors in machine learning algorithms [that] produce 
          unfair or discriminatory outcomes,” reflecting socio-economic, racial, and gender biases (Jonker and Rogers). 
          Left unchecked, such biases reinforce inequality and erode public trust. Compounding 
          this problem is <u>algorithmic opacity</u> – the “black box” nature of many AI systems. Algorithmic opacity can thus be understood as a form of testimonial silencing, 
          where individuals are excluded not only from decisions but from the reasoning that leads to them. Against this backdrop, this essay presents a 
          multi-dimensional strategy, spanning technical, ethical, and regulatory dimensions.
        </p>
        <p><a href="https://drive.google.com/file/d/1tZGgMmyxmT5thnBSx0k1zkeFc3ghrjgR/view?usp=sharing" target="_blank" class="preprint-btn">Read full essay</a></p>
      </article>

      <article>
        <h2><b>What is self-deceit?</b></h2>
        <p><b>Introduction</b></p>
        <p>
          <u>Self-deceit</u> – also called <u>self-deception</u> – is the act of forming or maintaining a belief that is false, often 
          against one's better knowledge, in order to avoid psychological discomfort (von Hippel et al.). Despite psychologists pointing out that self-deception is common, this 
          paradox has led some philosophers to question its validity. The essay explores logical paradoxes, cognitive mechanisms, and emotional consequences of self-deceit, 
          illustrating its role as both a defense mechanism and a philosophical puzzle.
        </p>
        <p><a href="https://drive.google.com/file/d/1QMBfvfdiev3Sycicm-9UoJenerDM0gRQ/view?usp=share_link" target="_blank" class="preprint-btn">Read full essay</a></p>
      </article>

      <article>
        <h2><b>In an increasingly AI-driven world, how is our ability to think for ourselves changing?</b></h2>
        <p><b>Introduction</b></p>
        <p>
          Human intelligence is multifaceted, and "thinking" involves creativity, moral reasoning, and flexibility. AI can process information rapidly, 
          but it cannot fully replicate ethical cognition or nuanced human awareness. This essay investigates how AI affects cognitive autonomy, memory, creativity, and moral judgment, 
          emphasizing the need for mindful engagement with technology to preserve independent thinking.
        </p>
        <p><a href="https://drive.google.com/file/d/1iekcoQCLRX0CegjozaOm_VeoT0hIzMeR/view?usp=share_link" target="_blank" class="preprint-btn">Read full essay</a></p>
      </article>
    </section>
  </main>

  <footer>
    <p>Copyright © 2025 Vanessa Matvei — All rights reserved.</p>
  </footer>

  <script src="script.js"></script>
</body>
</html>
