<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Vanessa Matvei | Articles</title>
  <link rel="stylesheet" type="text/css" href="style.css">
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
</head>
<body>
  <!-- Background canvas for petals -->
  <canvas id="bg-canvas"></canvas>

  <nav>
      <a href="index.html" class="nav-btn">About</a>        
      <a href="publications.html" class="nav-btn">Publications</a>      
      <a href="articles.html" class="nav-btn nav-active">Articles</a>
      <a href="contact.html" class="nav-btn">Contact</a>
  </nav>

  <header class="hero">
    <h1 class="hero-title">Articles & Essays</h1>
  </header>

  <main class="panel reveal">
    <section id="articles">
      <article>
        <h1><b>How can we deal with algorithmic bias and opacity?</b></h1>
        <p><b>Introduction</b></p>
        <p>	Nowadays, algorithms and artificial intelligence (AI) systems increasingly shape decisions in areas ranging from 
        credit scoring and hiring to news feeds and criminal justice. While efficient, these systems can encode and amplify 
        unfair biases. IBM defines <u>algorithmic bias</u> as “systematic errors in machine learning algorithms [that] produce 
        unfair or discriminatory outcomes,” reflecting socio-economic, racial, and gender biases (Jonker and Rogers). 
        Left unchecked, such biases reinforce inequality and erode public trust by perpetuating discrimination. Compounding 
        this problem is <u>algorithmic opacity</u> – the “black box” nature of many AI systems. When companies keep recommendation 
        logic secret, affected individuals and regulators cannot understand why outcomes occur. As philosopher Miranda Fricker 
        argues, <i>epistemic injustice </i>occurs “when someone is given less credence than they deserve,” and “when they are treated 
        unjustly” (Fricker). Algorithmic opacity can thus be understood as a form of testimonial silencing, where individuals are 
        excluded not only from decisions but from the reasoning that leads to them. Against this backdrop, this essay presents a 
        multi-dimensional strategy, spanning technical, ethical, and regulatory dimensions to theoretically address algorithmic 
        bias and opacity.
        </p>
        <p><a href="https://drive.google.com/file/d/1tZGgMmyxmT5thnBSx0k1zkeFc3ghrjgR/view?usp=sharing" target="_blank" class="preprint-btn">Read full essay</a></p>
      </article>

      <article>
        <h1><b>What is self-deceit?</b></h1>
        <p><b>Introduction</b></p>
        <p><u>
          Self-deceit</u> – also called <u>self-deception</u> – is the act of forming or maintaining a belief that is false, often 
          against one's better knowledge, in order to avoid psychological discomfort (von Hippel et al.). Defined this way, it appears 
          both familiar and straightforward. Yet there is a theoretical contradiction beneath it: how can someone <i>know</i> and <i>not know</i> 
          the exact truth at the same time? Despite psychologists pointing out that self-deception is a common aspect of human behavior, this 
          paradox has led some philosophers to question its validity. The conflict is obvious: why is self-deception so prevalent in real 
          life if it is illogical in theory? Are the mechanisms completely unconscious, or do we intentionally fool ourselves? More significantly, 
          what moral consequences result from this kind of internal deception?
        </p>

        <p>
          To address these questions, this essay looks at self-deceit from both philosophical and psychological perspectives. First, I analyze the 
          logical paradox at its core. Second, I investigate the cognitive and emotional mechanisms that enable individuals to deceive themselves, 
          often without being fully aware of it. Drawing on real-world examples and research, I argue that self-deceit functions as a motivated false belief: 
          it is both a defense mechanism and a philosophical puzzle – one that helps preserve emotional stability but at the cost of autonomy, clarity, and, 
          sometimes, truth.
        </p>

        <p><a href="https://drive.google.com/file/d/1QMBfvfdiev3Sycicm-9UoJenerDM0gRQ/view?usp=share_link" target="_blank" class="preprint-btn">Read full essay</a></p>
      </article>

      <article>
        <h1><b>In an increasingly AI-driven world, how is our ability to think for ourselves changing?</b></h1>
        <p><b>Introduction</b></p>
        <p>The answer to how artificial intelligence (AI) impacts our independent thinking depends on our definition of "thinking ability." 
        Psychologists emphasize that human intelligence is multifaceted. For example, <i>Sternberg's Triarchic Theory</i> distinguishes between analytical, 
        creative, and practical intelligence ("Learning and Intelligence"). Again, the areas of linguistic, logical-mathematical, spatial, interpersonal 
        (social understanding), and intrapersonal (self-reflection) intelligence are all identified in <i>Howard Gardner's Multiple Intelligences</i> ("Learning and Intelligence"). 
        In terms of these frameworks, "thinking" is not simply data processing; it involves moral reasoning, creativity, empathy, and flexibility. While machine learning is 
        improving significantly in identifying patterns, imitating human awareness and context flexibility remains a challenge. So, the question is not whether AI can think 
        and process information but whether it can match human creative and ethical cognition. With such complexities in mind, this essay will investigate how AI simultaneously 
        affects and influences cognitive autonomy in human beings – through memory, creativity, and moral judgment – and will emphasize the urgency of mindful engagement with technology 
        to preserve independent thinking.</p>
        <p><a href="https://drive.google.com/file/d/1iekcoQCLRX0CegjozaOm_VeoT0hIzMeR/view?usp=share_link" target="_blank" class="preprint-btn">Read full essay</a></p>
      </article>
    </section>
  </main>

  <footer>
    <p>Copyright © 2025 Vanessa Matvei — All rights reserved.</p>
  </footer>

  <script src="script.js"></script>
</body>
</html>
